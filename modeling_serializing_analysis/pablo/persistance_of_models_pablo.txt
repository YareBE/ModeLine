Joblib

Joblib is a library designed for pipelining and efficient disk-caching of Python objects, with function like joblib.dump() / joblib.load() that are optimized for objects that contain large NumPy arrays. This makes joblib particularly effective for machine-learning models whose parameters and internals are stored as dense numerical arrays. 

For objects dominated by large NumPy arrays, joblib usually is more efficient and produces smaller disk usage when using its built-in compression options. However, enabling compression reduces load speed (trading time for space). Joblib also supports several compressors and provides examples comparing speed vs size. 

Joblib is often the better choice for storing a LinearRegression model that contains NumPy arrays, because it handles contiguous numerical arrays efficiently and can memory-map large arrays on load to avoid copying into RAM. If a model is large (big coefficient matrices), joblib tends to be preferable. 

Joblib uses the pickle model underneath, so it is broadly compatible with Python objects, but it adds useful features (array-aware storage, compression). It requires little code to use and integrates well with NumPy/scikit-learn workflows. 


Pickle

The pickle module is the Python standard library’s general object serialization tool. It implements binary protocols to convert arbitrary Python object hierarchies to/from byte streams. Pickle is the baseline for many persistence tools and is suitable for a wide variety of Python objects. 

Pickle is a general-purpose serializer. For many small/heterogeneous Python objects, pickle can be faster than joblib because it writes objects as a single stream. However, for large  numeric arrays, pickle typically produces larger files and slower I/O than joblib’s format. In short: pickle is often faster for small Python-object dominated structures, but not as space-efficient or I/O-fast for large numeric arrays. 

Pickle works for storing LinearRegression objects, and it’s widely used for simple persistence. If a regression model is small (few parameters) and the objective is a dependency-free approach, pickle is fine. If the model includes large NumPy arrays or you want better array handling, joblib is typically better. 

Pickle is available in the Python standard library and is extremely simple to use (pickle.dump / pickle.load). Compatibility across Python versions can be brittle: different Python versions and different library versions can cause unpickling errors for complex objects. 


EXAMPLE OF CODE, SAVING MODELS
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
import pickle
import joblib

# Train a tiny linear model (example)
X, y = make_regression(n_samples=200, n_features=10, noise=0.1, random_state=0)
model = LinearRegression().fit(X, y)

# --- Save with pickle ---
with open("linreg_model.pkl", "wb") as f:
    # protocol=pickle.HIGHEST_PROTOCOL recommended for efficiency
    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)

# --- Load with pickle ---
with open("linreg_model.pkl", "rb") as f:
    model_pickle = pickle.load(f)

# --- Save with joblib ---
# compress=0 means no compression (fast); compress=3 (or higher) trades time for smaller size
joblib.dump(model, "linreg_model.joblib", compress=0)

# --- Load with joblib ---
model_joblib = joblib.load("linreg_model.joblib")

# Quick check (they should predict the same)
import numpy as np
x0 = X[:5]
print("pickle preds:", np.round(model_pickle.predict(x0), 6))
print("joblib preds:", np.round(model_joblib.predict(x0), 6))

